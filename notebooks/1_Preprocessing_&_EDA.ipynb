{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSBhiC6RXOTb"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base (Python 3.13.5)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bvwORtg2XbLO",
        "outputId": "76f3282c-6a81-4ec8-96f4-24854fd323e1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cTWFw-ZXf8e",
        "outputId": "ff2aad31-e5da-4fb1-be2c-89882986c527"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9wbrF_9XoYt",
        "outputId": "8ac9fab1-f2d0-4119-c241-252a1124c680"
      },
      "outputs": [],
      "source": [
        "df.sample()['clean_comment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4ADZSXHYWT6",
        "outputId": "c7b65c75-96c3-4e6f-fcf8-e4ccd29fe221"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "J2D2r1FZYywQ",
        "outputId": "0ed0a17a-288c-4a7c-b79d-da53d3587844"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "XIfr_vWeFwcx",
        "outputId": "297f2d38-6751-4139-b80e-3e3d362659fd"
      },
      "outputs": [],
      "source": [
        "df[df['clean_comment'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "3Gz1hROGQhcg",
        "outputId": "fa4bb8c7-c374-4b7f-d9f2-78d48b7e4ea7"
      },
      "outputs": [],
      "source": [
        "df[df['clean_comment'].isna()]['category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir5BqpqTQS2b"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqzmpFcYGaRK",
        "outputId": "90297694-9200-48a0-8c9c-5acb58fb6a36"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oPLdUA02GmX5",
        "outputId": "84c19b3b-5c41-43fe-cf20-3296d147287d"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTTofRWcG_ZB"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOWZ6DtDQyIE",
        "outputId": "3f0b5cef-e3c9-4951-da19-8c5b6984caf3"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "R9TTpUpj_d_7",
        "outputId": "d0a06f65-4bc4-4c5a-d350-031ac04813c1"
      },
      "outputs": [],
      "source": [
        "df[(df['clean_comment'].str.strip() == '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw0vLDiEQ6Cq"
      },
      "outputs": [],
      "source": [
        "df = df[~(df['clean_comment'].str.strip() == '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6WoKLuXoSMEi",
        "outputId": "d13dbad5-9a6f-4a5e-cb27-582763375964"
      },
      "outputs": [],
      "source": [
        "# Convert the 'clean_comment' column to lowercase\n",
        "df['clean_comment'] = df['clean_comment'].str.lower()\n",
        "\n",
        "# Verify the transformation by displaying the first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "IHFXwtlSAS4w",
        "outputId": "d5ca44a6-3073-4927-95b0-38d1864e6fcc"
      },
      "outputs": [],
      "source": [
        "df[df['clean_comment'].apply(lambda x: x.endswith(' ') or x.startswith(' '))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAV6fb4u_8vJ",
        "outputId": "80575591-d063-46c3-ff96-304e36397a2c"
      },
      "outputs": [],
      "source": [
        "# Remove trailing and leading whitespaces from the 'clean_comment' column\n",
        "df['clean_comment'] = df['clean_comment'].str.strip()\n",
        "\n",
        "# Verify the transformation by checking for any remaining trailing whitespaces\n",
        "df['clean_comment'].apply(lambda x: x.endswith(' ') or x.startswith(' ')).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "258v-_Ftr4ge",
        "outputId": "d9aacf2d-bca4-45bc-e858-fa5d7afd036a"
      },
      "outputs": [],
      "source": [
        "# Identify comments containing URLs\n",
        "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "comments_with_urls = df[df['clean_comment'].str.contains(url_pattern, regex=True)]\n",
        "\n",
        "# Display the comments containing URLs\n",
        "comments_with_urls.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7urVyCPFsJoX",
        "outputId": "566addda-e19a-4892-b8e0-1d27a2a43eb7"
      },
      "outputs": [],
      "source": [
        "# Identify comments containing new line characters\n",
        "comments_with_newline = df[df['clean_comment'].str.contains('\\n')]\n",
        "\n",
        "# Display the comments containing new line characters\n",
        "comments_with_newline.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "1u10XzZ6sXMk",
        "outputId": "4864b59d-b706-4bd7-958f-e4c3b43964c9"
      },
      "outputs": [],
      "source": [
        "# Remove new line characters from the 'clean_comment' column\n",
        "df['clean_comment'] = df['clean_comment'].str.replace('\\n', ' ', regex=True)\n",
        "\n",
        "# Verify the transformation by checking for any remaining new lines\n",
        "comments_with_newline_remaining = df[df['clean_comment'].str.contains('\\n')]\n",
        "comments_with_newline_remaining\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1WOgmhW11i"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "16vGxnIDVveC",
        "outputId": "15c4a9fb-1660-4256-81cc-7470f7608879"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# distribution of classes\n",
        "\n",
        "sns.countplot(data=df,x=\"category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Gpk-epp2W99t",
        "outputId": "912c639d-7593-4a50-9221-93ac1e04766a"
      },
      "outputs": [],
      "source": [
        "# frequency distribution of sentiments\n",
        "\n",
        "df['category'].value_counts(normalize=True).mul(100).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NtZxvPPXLGB"
      },
      "outputs": [],
      "source": [
        "df['word_count'] = df['clean_comment'].apply(lambda x: len(x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QJPYTSYhYmX9",
        "outputId": "08b00b15-725b-4a59-b61a-c83a18e73bba"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "QlvgPXhcYz5D",
        "outputId": "7fcc5aca-dc2a-46bb-ddfd-9946be99d3a6"
      },
      "outputs": [],
      "source": [
        "df['word_count'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "V9CuIvWDjZOh",
        "outputId": "c04fdc90-7cf2-4e89-fe04-d3f16ec51fd8"
      },
      "outputs": [],
      "source": [
        "sns.displot(df['word_count'], kde=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "avdlB78Ljmfy",
        "outputId": "d8b8dc02-39c0-437a-eb4e-6233ecbf5628"
      },
      "outputs": [],
      "source": [
        "# Create the figure and axes\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot KDE for category 1\n",
        "sns.kdeplot(df[df['category'] == 1]['word_count'], label='Positive', fill=True)\n",
        "\n",
        "# Plot KDE for category 0\n",
        "sns.kdeplot(df[df['category'] == 0]['word_count'], label='Neutral', fill=True)\n",
        "\n",
        "# Plot KDE for category -1\n",
        "sns.kdeplot(df[df['category'] == -1]['word_count'], label='Negative', fill=True)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Word Count Distribution by Category')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZpwFAg-miya"
      },
      "source": [
        "**Positive comments (category 1)**: These tend to have a wider spread in word count, indicating that longer comments are more common in positive sentiments.<br>\n",
        "**Neutral comments (category 0)**: The distribution shows a relatively lower frequency and is more concentrated around shorter comments compared to positive or negative ones.<br>\n",
        "**Negative comments (category -1)**: These comments have a distribution somewhat similar to positive comments but with a smaller proportion of longer comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "S-YOr_QZj2yG",
        "outputId": "7a0935f4-586e-4453-df9b-2677ccda137f"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(df['word_count'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "pA2HY6Z7kX_W",
        "outputId": "75ecf3e2-d20b-4465-bd7d-25e576ee07a2"
      },
      "outputs": [],
      "source": [
        "# Create a boxplot for the 'wordcount' column categorized by 'category'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df, x='category', y='word_count')\n",
        "plt.title('Boxplot of Word Count by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Word Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2A1i2nGnO2U"
      },
      "source": [
        "**Positive comments (category 1)**: The median word count is relatively high, and there are several outliers with longer comments, indicating that positive comments tend to be more verbose.<br>\n",
        "**Neutral comments (category 0)**: The median word count is the lowest, with a tighter interquartile range (IQR), suggesting that neutral comments are generally shorter.<br>\n",
        "**Negative comments (category -1)**: The word count distribution is similar to positive comments but with a slightly lower median and fewer extreme outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "s_RsxaN4nCBJ",
        "outputId": "7dc0cae8-4e55-4369-b9f0-78fe16e51d7b"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot between 'category' and 'wordcount'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='category', y='word_count', alpha=0.5)\n",
        "plt.title('Scatterplot of Word Count by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Word Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "XnQ7_euWnwch",
        "outputId": "1504b4b0-03e4-4122-bedd-c8df92a947df"
      },
      "outputs": [],
      "source": [
        "# median word counts among sentiments\n",
        "\n",
        "sns.barplot(df,x='category',y='word_count',estimator='median')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyrx6Q1no6pn",
        "outputId": "b4282132-f8b6-46a0-e0f3-07a68caaff42"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R05FaZ0psQc",
        "outputId": "9d8f050a-1ae8-4201-9b69-744efaef28b7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Create a new column 'num_stop_words' by counting the number of stopwords in each comment\n",
        "df['num_stop_words'] = df['clean_comment'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Vg2g57Bnpz2T",
        "outputId": "b8bffd85-ab75-4d1f-c589-6048c51844e2"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LhAxA2Jep2VI",
        "outputId": "75f61b14-6e47-4c9b-94ea-348b2a94dc4d"
      },
      "outputs": [],
      "source": [
        "# Create a distribution plot (displot) for the 'num_stop_words' column\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['num_stop_words'], kde=True)\n",
        "plt.title('Distribution of Stop Word Count in Comments')\n",
        "plt.xlabel('Number of Stop Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "oyMgv4DkqIrL",
        "outputId": "f197a0bc-9e51-4fec-8715-a4bbe7b1aa10"
      },
      "outputs": [],
      "source": [
        "# Create the figure and axes\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot KDE for category 1\n",
        "sns.kdeplot(df[df['category'] == 1]['num_stop_words'], label='Positive', fill=True)\n",
        "\n",
        "# Plot KDE for category 0\n",
        "sns.kdeplot(df[df['category'] == 0]['num_stop_words'], label='Neutral', fill=True)\n",
        "\n",
        "# Plot KDE for category -1\n",
        "sns.kdeplot(df[df['category'] == -1]['num_stop_words'], label='Negative', fill=True)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Num stop words Distribution by Category')\n",
        "plt.xlabel('Stop word count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "3qFE2DNTqkhn",
        "outputId": "921195d9-8e3f-4ab5-862b-32c2746c909e"
      },
      "outputs": [],
      "source": [
        "# median word counts among sentiments\n",
        "\n",
        "sns.barplot(df,x='category',y='num_stop_words',estimator='median')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "XbUslXk4q0dX",
        "outputId": "4498ca05-4c73-4e99-d45d-a0a35dd41757"
      },
      "outputs": [],
      "source": [
        "# Create a frequency distribution of stop words in the 'clean_comment' column\n",
        "from collections import Counter\n",
        "\n",
        "# Extract all stop words from the comments using the previously defined 'common_stopwords'\n",
        "all_stop_words = [word for comment in df['clean_comment'] for word in comment.split() if word in stop_words]\n",
        "\n",
        "# Count the most common stop words\n",
        "most_common_stop_words = Counter(all_stop_words).most_common(25)\n",
        "\n",
        "# Convert the most common stop words to a DataFrame for plotting\n",
        "top_25_df = pd.DataFrame(most_common_stop_words, columns=['stop_word', 'count'])\n",
        "\n",
        "# Create the barplot for the top 25 most common stop words\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=top_25_df, x='count', y='stop_word', palette='viridis')\n",
        "plt.title('Top 25 Most Common Stop Words')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Stop Word')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "44DS1EQi25bq",
        "outputId": "a362e4d3-4238-4cc4-a7ab-c01a929ea6d0"
      },
      "outputs": [],
      "source": [
        "df['num_chars'] = df['clean_comment'].apply(len)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "myvWXXhn3xKg",
        "outputId": "94eb1a77-fd21-4b78-9def-0cfcaa0f854f"
      },
      "outputs": [],
      "source": [
        "df['num_chars'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dd66MS44YsQ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Combine all comments into one large string\n",
        "all_text = ' '.join(df['clean_comment'])\n",
        "\n",
        "# Count the frequency of each character\n",
        "char_frequency = Counter(all_text)\n",
        "\n",
        "# Convert the character frequency into a DataFrame for better display\n",
        "char_frequency_df = pd.DataFrame(char_frequency.items(), columns=['character', 'frequency']).sort_values(by='frequency', ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApclVGa_Bv8",
        "outputId": "7135ceb7-be28-49fc-fc74-2f2cd1daff47"
      },
      "outputs": [],
      "source": [
        "char_frequency_df['character'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H1oZxrzZ44Bb",
        "outputId": "bfa1d145-fc53-4695-952e-fa593a6f243c"
      },
      "outputs": [],
      "source": [
        "char_frequency_df.tail(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LC2pA8LRsAnL",
        "outputId": "f7ed46f5-2aa0-4b93-f4de-e758efe521b0"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'num_punctuation_chars' to count punctuation characters in each comment\n",
        "df['num_punctuation_chars'] = df['clean_comment'].apply(\n",
        "    lambda x: sum([1 for char in x if char in '.,!?;:\"\\'()[]{}-'])\n",
        ")\n",
        "\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "fhD0F3HcvWYq",
        "outputId": "6a6254b2-6cca-4523-f3ad-c66f3e8de8b6"
      },
      "outputs": [],
      "source": [
        "df['num_punctuation_chars'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "oBn16zTZvbiB",
        "outputId": "35fe7037-20b0-4ee0-d442-e0e6e4ffc0c2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a function to extract the top 25 bigrams\n",
        "def get_top_ngrams(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Get the top 25 bigrams\n",
        "top_25_bigrams = get_top_ngrams(df['clean_comment'], 25)\n",
        "\n",
        "# Convert the bigrams into a DataFrame for plotting\n",
        "top_25_bigrams_df = pd.DataFrame(top_25_bigrams, columns=['bigram', 'count'])\n",
        "\n",
        "# Plot the countplot for the top 25 bigrams\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=top_25_bigrams_df, x='count', y='bigram', palette='magma')\n",
        "plt.title('Top 25 Most Common Bigrams')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Bigram')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "G9Z8sVULv18y",
        "outputId": "113f7bd7-1b2c-471f-8089-cd69d3e0cf63"
      },
      "outputs": [],
      "source": [
        "# Create a function to extract the top 25 trigrams\n",
        "def get_top_trigrams(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Get the top 25 trigrams\n",
        "top_25_trigrams = get_top_trigrams(df['clean_comment'], 25)\n",
        "\n",
        "# Convert the trigrams into a DataFrame for plotting\n",
        "top_25_trigrams_df = pd.DataFrame(top_25_trigrams, columns=['trigram', 'count'])\n",
        "\n",
        "# Plot the countplot for the top 25 trigrams\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=top_25_trigrams_df, x='count', y='trigram', palette='coolwarm')\n",
        "plt.title('Top 25 Most Common Trigrams')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Trigram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UySAlrxzGQ0e"
      },
      "outputs": [],
      "source": [
        "# Remove non-English characters from the 'clean_comment' column\n",
        "# Keeping only standard English letters, digits, and common punctuation\n",
        "import re\n",
        "\n",
        "df['clean_comment'] = df['clean_comment'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s!?.,]', '', str(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sUJgHRpmKlGc",
        "outputId": "e1021108-55bf-4333-e383-8a8b134e0c11"
      },
      "outputs": [],
      "source": [
        "all_text = ' '.join(df['clean_comment'])\n",
        "\n",
        "# Count the frequency of each character\n",
        "char_frequency = Counter(all_text)\n",
        "\n",
        "# Convert the character frequency into a DataFrame for better display\n",
        "char_frequency_df = pd.DataFrame(char_frequency.items(), columns=['character', 'frequency']).sort_values(by='frequency', ascending=False)\n",
        "\n",
        "char_frequency_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nqqsHg7fKu_-",
        "outputId": "8dfe3d22-d76f-4c46-a4e2-ccda543c202f"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3v0LQFwMGj8"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Defining stop words but keeping essential ones for sentiment analysis\n",
        "stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "\n",
        "# Remove stop words from 'clean_comment' column, retaining essential ones\n",
        "df['clean_comment'] = df['clean_comment'].apply(\n",
        "    lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jolJmr96Nxv1",
        "outputId": "5940113a-10ef-487e-d5b3-c45a6ab2bbf7"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "hDQRz_qrNzUl",
        "outputId": "468c8216-f3ce-4664-ff04-17e60cbe5578"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to the 'clean_comment_no_stopwords' column\n",
        "df['clean_comment'] = df['clean_comment'].apply(\n",
        "    lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "Wads6EfZQ9Ot",
        "outputId": "5a49f00b-41bb-4144-8444-19d29ac615a1"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_word_cloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "plot_word_cloud(df['clean_comment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "FzkM7n0dUaY4",
        "outputId": "954eeb91-3d82-4d00-c87f-c9a584da9217"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_word_cloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "plot_word_cloud(df[df['category'] == 1]['clean_comment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "hgjDPYosUmSl",
        "outputId": "432ebbbd-e554-4082-9960-a1635cee0a91"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_word_cloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "plot_word_cloud(df[df['category'] == 0]['clean_comment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "mewK5p4pUsaL",
        "outputId": "38c64136-bf78-43c4-a946-4ea08b1fc10e"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_word_cloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "plot_word_cloud(df[df['category'] == -1]['clean_comment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "spDMUnY9UwzS",
        "outputId": "850f3997-005a-4fbe-8107-4d80489565c5"
      },
      "outputs": [],
      "source": [
        "def plot_top_n_words(df, n=20):\n",
        "    \"\"\"Plot the top N most frequent words in the dataset.\"\"\"\n",
        "    # Flatten all words in the content column\n",
        "    words = ' '.join(df['clean_comment']).split()\n",
        "\n",
        "    # Get the top N most common words\n",
        "    counter = Counter(words)\n",
        "    most_common_words = counter.most_common(n)\n",
        "\n",
        "    # Split the words and their counts for plotting\n",
        "    words, counts = zip(*most_common_words)\n",
        "\n",
        "    # Plot the top N words\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(words))\n",
        "    plt.title(f'Top {n} Most Frequent Words')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_top_n_words(df, n=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "U41Lv_vwVEWc",
        "outputId": "e44b2e17-16bd-40e0-8d08-893cc59573d4"
      },
      "outputs": [],
      "source": [
        "def plot_top_n_words_by_category(df, n=20, start=0):\n",
        "    \"\"\"Plot the top N most frequent words in the dataset with stacked hue based on sentiment category.\"\"\"\n",
        "    # Flatten all words in the content column and count their occurrences by category\n",
        "    word_category_counts = {}\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        words = row['clean_comment'].split()\n",
        "        category = row['category']  # Assuming 'category' column exists for -1, 0, 1 labels\n",
        "\n",
        "        for word in words:\n",
        "            if word not in word_category_counts:\n",
        "                word_category_counts[word] = { -1: 0, 0: 0, 1: 0 }  # Initialize counts for each sentiment category\n",
        "\n",
        "            # Increment the count for the corresponding sentiment category\n",
        "            word_category_counts[word][category] += 1\n",
        "\n",
        "    # Get total counts across all categories for each word\n",
        "    total_word_counts = {word: sum(counts.values()) for word, counts in word_category_counts.items()}\n",
        "\n",
        "    # Get the top N most frequent words across all categories\n",
        "    most_common_words = sorted(total_word_counts.items(), key=lambda x: x[1], reverse=True)[start:start+n]\n",
        "    top_words = [word for word, _ in most_common_words]\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    word_labels = top_words\n",
        "    negative_counts = [word_category_counts[word][-1] for word in top_words]\n",
        "    neutral_counts = [word_category_counts[word][0] for word in top_words]\n",
        "    positive_counts = [word_category_counts[word][1] for word in top_words]\n",
        "\n",
        "    # Plot the stacked bar chart\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    bar_width = 0.75\n",
        "\n",
        "    # Plot negative, neutral, and positive counts in a stacked manner\n",
        "    plt.barh(word_labels, negative_counts, color='red', label='Negative (-1)', height=bar_width)\n",
        "    plt.barh(word_labels, neutral_counts, left=negative_counts, color='gray', label='Neutral (0)', height=bar_width)\n",
        "    plt.barh(word_labels, positive_counts, left=[i+j for i,j in zip(negative_counts, neutral_counts)], color='green', label='Positive (1)', height=bar_width)\n",
        "\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.title(f'Top {n} Most Frequent Words with Stacked Sentiment Categories')\n",
        "    plt.legend(title='Sentiment', loc='lower right')\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to show the highest frequency at the top\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plot_top_n_words_by_category(df, n=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39bUJsi0Vgix"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
